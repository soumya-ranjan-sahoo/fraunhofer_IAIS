{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "from spacy.lang.en import English\n",
    "import nltk\n",
    "\n",
    "### Changing the work directory ###\n",
    "\n",
    "\"\"\"\n",
    "Kindly change your working directory here \n",
    "\"\"\"\n",
    "os.chdir(\"D:\\\\UniSaarland\\\\incar-dialogue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uploading Datasets ###\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "valid_df = pd.read_csv(\"valid.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "DFs = [train_df,valid_df,test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThese functions will help to clean the corpus.\\n---NOT USED ---\\nI am not using these functions here as we will have to clean the KG as well in order to use the clean utterances as\\nthe entities in the kg have special characters and punctuations with them\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Utility functions for text cleaning ###\n",
    "\n",
    "\"\"\"\n",
    "These functions will help to clean the corpus.\n",
    "---NOT USED ---\n",
    "I am not using these functions here as we will have to clean the KG as well in order to use the clean utterances as\n",
    "the entities in the kg have special characters and punctuations with them\n",
    "\"\"\"\n",
    "\n",
    "# def clean_text(x):\n",
    "#     text = re.sub('(\\d+)','',x)   \n",
    "#     text = text.lower()\n",
    "#     return text\n",
    "# def remove_url(x):\n",
    "#     text = re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})\\/([a-zA-Z0-9_]+]*)',' ',x)\n",
    "#     return text\n",
    "# def remove_punct(x):\n",
    "#     text_without_puct = [t for t in x if t not in string.punctuation]\n",
    "#     text_without_puct = ''.join(text_without_puct)\n",
    "#     return text_without_puct\n",
    "\n",
    "### Cleaning the dataframes ###\n",
    "\n",
    "# for df in DFs:\n",
    "    \n",
    "#     df[\"utterance\"] = df[\"utterance\"].apply(clean_text)\n",
    "#     df[\"utterance\"] = df[\"utterance\"].apply(remove_url)\n",
    "#     df[\"utterance\"] = df[\"utterance\"].apply(remove_punct)\n",
    "#     df[\"context\"] = df[\"context\"].apply(clean_text)\n",
    "#     df[\"context\"] = df[\"context\"].apply(remove_url)\n",
    "#     df[\"context\"] = df[\"context\"].apply(remove_punct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(kg_triples,entList):\n",
    "    \"\"\"\n",
    "    Utility function for PredictEntities\n",
    "    Input : knowledge-graph's triples and extracted triples from the paragraph\n",
    "    Returns : Predicted triples \n",
    "    \"\"\"\n",
    "    extracted_triples =[]\n",
    "    for ent in entList:\n",
    "        for triple in kg_triples:\n",
    "            if ent == triple[0]:\n",
    "                extracted_triples.append(triple)\n",
    "    return extracted_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def PredictTriples(dataframe):\n",
    "    \"\"\"\n",
    "    This is the main function that performs the triple predictions after extracting triples from the utterances \n",
    "    Input : The dataframe from which triples are to be extracted an is to be evaluated\n",
    "    Returns : Predicted triples from each row of the dataframe \n",
    "    \n",
    "    Note : Please not that as recommended in the problem, I have only used utterance, context and kg as inputs \n",
    "    \"\"\"\n",
    "    \n",
    "    NER = spacy.load(\"en_core_web_sm\")\n",
    "    df = dataframe\n",
    "    predicted_labels=[]\n",
    "    for index, row in df.iterrows():\n",
    "        paragraph = []\n",
    "        row[\"context\"] = ast.literal_eval(row[\"context\"])\n",
    "        for utterance in row[\"context\"]:\n",
    "            paragraph.append(utterance)\n",
    "        paragraph.append(row[\"utterance\"])\n",
    "        text = ' '.join(map(str, paragraph))\n",
    "        text1 = NER(text)\n",
    "        entList = []\n",
    "        for word in text1.ents:\n",
    "            entList.append(word.text)\n",
    "        kg_triples = ast.literal_eval(row[\"kg\"])\n",
    "        predicted_labels.append(match(kg_triples,entList)) # function call to match()\n",
    "        if index % 100 == 0 and index != 0: \n",
    "            print(\"{} values extracted!\".format(index))\n",
    "    return predicted_labels   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateAccuracy(predicted_labels,ground_labels):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function calculates instance/row wise score(accuracy)\n",
    "    Input : predicted-labels and ground-lables\n",
    "    Returns : A list with accuracy for each row \n",
    "    \"\"\"\n",
    "    instance_accuracy = []\n",
    "    for idx in range(len(predicted_labels)): \n",
    "        ground_val = ast.literal_eval(ground_labels.iloc[idx])\n",
    "        intersection_vals = [set(tuple(row) for row in predicted_labels[idx] ) & set(tuple(row) for row in ground_val)]\n",
    "        if intersection_vals == [set()]:\n",
    "            instance_accuracy.append(0)\n",
    "        else:\n",
    "            if len(ground_val) == 0:\n",
    "                instance_accuracy.append(0)\n",
    "            else:\n",
    "                score = (len(intersection_vals))/len(ground_val)\n",
    "                instance_accuracy.append(score*100)\n",
    "\n",
    "    return instance_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Processing and evaluating train_df #####\n",
      "100 values extracted!\n",
      "200 values extracted!\n",
      "300 values extracted!\n",
      "400 values extracted!\n",
      "500 values extracted!\n",
      "600 values extracted!\n",
      "700 values extracted!\n",
      "800 values extracted!\n",
      "900 values extracted!\n",
      "1000 values extracted!\n",
      "1100 values extracted!\n",
      "1200 values extracted!\n",
      "1300 values extracted!\n",
      "1400 values extracted!\n",
      "1500 values extracted!\n",
      "1600 values extracted!\n",
      "1700 values extracted!\n",
      "1800 values extracted!\n",
      "1900 values extracted!\n",
      "2000 values extracted!\n",
      "2100 values extracted!\n",
      "2200 values extracted!\n",
      "2300 values extracted!\n",
      "2400 values extracted!\n",
      "2500 values extracted!\n",
      "2600 values extracted!\n",
      "2700 values extracted!\n",
      "2800 values extracted!\n",
      "2900 values extracted!\n",
      "3000 values extracted!\n",
      "3100 values extracted!\n",
      "3200 values extracted!\n",
      "3300 values extracted!\n",
      "3400 values extracted!\n",
      "3500 values extracted!\n",
      "3600 values extracted!\n",
      "3700 values extracted!\n",
      "3800 values extracted!\n",
      "3900 values extracted!\n",
      "4000 values extracted!\n",
      "4100 values extracted!\n",
      "4200 values extracted!\n",
      "4300 values extracted!\n",
      "4400 values extracted!\n",
      "4500 values extracted!\n",
      "4600 values extracted!\n",
      "4700 values extracted!\n",
      "4800 values extracted!\n",
      "4900 values extracted!\n",
      "5000 values extracted!\n",
      "5100 values extracted!\n",
      "5200 values extracted!\n",
      "5300 values extracted!\n",
      "5400 values extracted!\n",
      "5500 values extracted!\n",
      "5600 values extracted!\n",
      "5700 values extracted!\n",
      "5800 values extracted!\n",
      "5900 values extracted!\n",
      "6000 values extracted!\n",
      "6100 values extracted!\n",
      "6200 values extracted!\n",
      "The average accuracy for the dataset train_df is 1.36 %.\n",
      "##### Processing and evaluating valid_df #####\n",
      "100 values extracted!\n",
      "200 values extracted!\n",
      "300 values extracted!\n",
      "400 values extracted!\n",
      "500 values extracted!\n",
      "600 values extracted!\n",
      "700 values extracted!\n",
      "The average accuracy for the dataset valid_df is 1.81 %.\n",
      "##### Processing and evaluating test_df #####\n",
      "100 values extracted!\n",
      "200 values extracted!\n",
      "300 values extracted!\n",
      "400 values extracted!\n",
      "500 values extracted!\n",
      "600 values extracted!\n",
      "700 values extracted!\n",
      "800 values extracted!\n",
      "The average accuracy for the dataset test_df is 1.42 %.\n"
     ]
    }
   ],
   "source": [
    "### Driver Section ###\n",
    "\n",
    "\"\"\"\n",
    "Please select the dataframe you want to evaluate from - train_df, valid_df,test_df\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"##### Processing and evaluating train_df #####\")\n",
    "df = train_df\n",
    "df_name = \"train_df\"\n",
    "predicted_labels = PredictTriples(df)\n",
    "ground_labels = df.labels\n",
    "if len(predicted_labels) == len(ground_labels):\n",
    "    instance_accuracy = CalculateAccuracy(predicted_labels,ground_labels)\n",
    "else :\n",
    "    print(\"Dimension mismatch between the predicted and ground labels\")\n",
    "average_accuracy = round((sum(instance_accuracy)/len(instance_accuracy)),2)\n",
    "\n",
    "#print(predicted_labels)\n",
    "#print(instance_accuracy)\n",
    "print(\"The average accuracy for the dataset {} is {} %.\".format(df_name,average_accuracy))\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "print(\"##### Processing and evaluating valid_df #####\")\n",
    "df = valid_df\n",
    "df_name = \"valid_df\"\n",
    "predicted_labels = PredictTriples(df)\n",
    "ground_labels = df.labels\n",
    "if len(predicted_labels) == len(ground_labels):\n",
    "    instance_accuracy = CalculateAccuracy(predicted_labels,ground_labels)\n",
    "else :\n",
    "    print(\"Dimension mismatch between the predicted and ground labels\")\n",
    "average_accuracy = round((sum(instance_accuracy)/len(instance_accuracy)),2)\n",
    "\n",
    "#print(predicted_labels)\n",
    "#print(instance_accuracy)\n",
    "print(\"The average accuracy for the dataset {} is {} %.\".format(df_name,average_accuracy))\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "\n",
    "print(\"##### Processing and evaluating test_df #####\")\n",
    "df = test_df\n",
    "df_name = \"test_df\"\n",
    "predicted_labels = PredictTriples(df)\n",
    "ground_labels = df.labels\n",
    "if len(predicted_labels) == len(ground_labels):\n",
    "    instance_accuracy = CalculateAccuracy(predicted_labels,ground_labels)\n",
    "else :\n",
    "    print(\"Dimension mismatch between the predicted and ground labels\")\n",
    "average_accuracy = round((sum(instance_accuracy)/len(instance_accuracy)),2)\n",
    "\n",
    "#print(predicted_labels)\n",
    "#print(instance_accuracy)\n",
    "print(\"The average accuracy for the dataset {} is {} %.\".format(df_name,average_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
