{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "### Changing the work directory ###\n",
    "\n",
    "\"\"\"\n",
    "Kindly change your working directory here \n",
    "\"\"\"\n",
    "os.chdir(\"D:\\\\UniSaarland\\\\HiWi_Fraunhofer\\\\incar-dialogue\") # kindly chnage the directory here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uploading Datasets ###\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "valid_df = pd.read_csv(\"valid.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "DFs = [train_df,valid_df,test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThese functions will help to clean the corpus.\\n---NOT USED ---\\nI am not using these functions here as we will have to clean the KG as well in order to use the clean utterances as\\nthe entities in the kg have special characters and punctuations with them\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Utility functions for text cleaning ###\n",
    "\n",
    "\"\"\"\n",
    "These functions will help to clean the corpus.\n",
    "---NOT USED ---\n",
    "I am not using these functions here as we will have to clean the KG as well in order to use the clean utterances as\n",
    "the entities in the kg have special characters and punctuations with them\n",
    "\"\"\"\n",
    "\n",
    "# def clean_text(x):\n",
    "#     text = re.sub('(\\d+)','',x)   \n",
    "#     text = text.lower()\n",
    "#     return text\n",
    "# def remove_url(x):\n",
    "#     text = re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})\\/([a-zA-Z0-9_]+]*)',' ',x)\n",
    "#     return text\n",
    "# def remove_punct(x):\n",
    "#     text_without_puct = [t for t in x if t not in string.punctuation]\n",
    "#     text_without_puct = ''.join(text_without_puct)\n",
    "#     return text_without_puct\n",
    "\n",
    "### Cleaning the dataframes ###\n",
    "\n",
    "# for df in DFs:\n",
    "    \n",
    "#     df[\"utterance\"] = df[\"utterance\"].apply(clean_text)\n",
    "#     df[\"utterance\"] = df[\"utterance\"].apply(remove_url)\n",
    "#     df[\"utterance\"] = df[\"utterance\"].apply(remove_punct)\n",
    "#     df[\"context\"] = df[\"context\"].apply(clean_text)\n",
    "#     df[\"context\"] = df[\"context\"].apply(remove_url)\n",
    "#     df[\"context\"] = df[\"context\"].apply(remove_punct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utility functions for triple extraction ###\n",
    "\n",
    "\"\"\"\n",
    "This is sourced from https://www.kaggle.com/rahulvks/knowledge-graphs-information-extraction\n",
    "Note : After a lot of experiments around triple extractors with custom modifications, this seemed to be the best choice \n",
    "but it fails on longer texts very badly. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "from spacy.lang.en import English\n",
    "import nltk\n",
    "def getSentences(text):\n",
    "    nlp = English()\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "    document = nlp(text)\n",
    "    return [sent.string.strip() for sent in document.sents]\n",
    "\n",
    "def printToken(token):\n",
    "    #print(token.text, \"->\", token.dep_)\n",
    "    pass\n",
    "def appendChunk(original, chunk):\n",
    "    return original + ' ' + chunk\n",
    "def isRelationCandidate(token):\n",
    "    deps = [\"ROOT\", \"adj\", \"attr\", \"agent\", \"amod\"]\n",
    "    return any(subs in token.dep_ for subs in deps)\n",
    "def isConstructionCandidate(token):\n",
    "    deps = [\"compound\", \"prep\", \"conj\", \"mod\"]\n",
    "    return any(subs in token.dep_ for subs in deps)\n",
    "def processSubjectObjectPairs(tokens):\n",
    "    subject = ''\n",
    "    object = ''\n",
    "    relation = ''\n",
    "    subjectConstruction = ''\n",
    "    objectConstruction = ''\n",
    "    for token in tokens:\n",
    "       # printToken(token)\n",
    "        if \"punct\" in token.dep_:\n",
    "            continue\n",
    "        if isRelationCandidate(token):\n",
    "            relation = appendChunk(relation, token.lemma_)\n",
    "        if isConstructionCandidate(token):\n",
    "            if subjectConstruction:\n",
    "                subjectConstruction = appendChunk(subjectConstruction, token.text)\n",
    "            if objectConstruction:\n",
    "                objectConstruction = appendChunk(objectConstruction, token.text)\n",
    "        if \"subj\" in token.dep_:\n",
    "            subject = appendChunk(subject, token.text)\n",
    "            subject = appendChunk(subjectConstruction, subject)\n",
    "            subjectConstruction = ''\n",
    "        if \"obj\" in token.dep_:\n",
    "            object = appendChunk(object, token.text)\n",
    "            object = appendChunk(objectConstruction, object)\n",
    "            objectConstruction = ''\n",
    "            \n",
    "    #print (subject.strip(), \",\", relation.strip(), \",\", object.strip())\n",
    "    return [subject.strip(), relation.strip(), object.strip()]\n",
    "\n",
    "\n",
    "def processSentence(sentence):\n",
    "    nlp_model = spacy.load('en_core_web_sm')\n",
    "    tokens = nlp_model(sentence)\n",
    "    return processSubjectObjectPairs(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(list1, list2):\n",
    "    \"\"\"\n",
    "    Utility function for PredictEntities\n",
    "    Input : knowledge-graph's triples and extracted triples from the paragraph\n",
    "    Returns : Predicted triples \n",
    "    \"\"\"\n",
    "    \n",
    "    return [l1 for l1 in list1 if any(l2[0]==l1[0] for l2 in list2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictTriples(dataframe):\n",
    "    \"\"\"\n",
    "    This is the main function that performs the triple predictions after extracting triples from the utterances \n",
    "    Input : The dataframe from which triples are to be extracted an is to be evaluated\n",
    "    Returns : Predicted triples from each row of the dataframe \n",
    "    \n",
    "    Note : Please not that as recommended in the problem, I have only used utterance, context and kg as inputs \n",
    "    \"\"\"\n",
    "    \n",
    "    df = dataframe\n",
    "    predicted_labels=[]\n",
    "    for index, row in df.iterrows():\n",
    "        paragraph = []\n",
    "        row[\"context\"] = ast.literal_eval(row[\"context\"])\n",
    "        for utterance in row[\"context\"]:\n",
    "            paragraph.append(utterance)\n",
    "        paragraph.append(row[\"utterance\"])\n",
    "        text = ' '.join(map(str, paragraph))\n",
    "        sentences = getSentences(text)\n",
    "        nlp_model = spacy.load('en_core_web_sm')\n",
    "        triples = []\n",
    "        for sentence in sentences:\n",
    "            triples.append(processSentence(sentence))\n",
    "        #print(index,triples)\n",
    "        kg_entities = ast.literal_eval(row[\"kg\"])\n",
    "        #print(type(kg_entities),type(triples))\n",
    "        predicted_labels.append(match(kg_entities,triples)) # function call to match()\n",
    "        if index % 100 == 0 and index != 0: \n",
    "            print(\"{} values extracted!\".format(index))\n",
    "    return predicted_labels           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateAccuracy(predicted_labels,ground_labels):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function calculates instance/row wise score(accuracy)\n",
    "    Input : predicted-labels and ground-lables\n",
    "    Returns : A list with accuracy for each row \n",
    "    \"\"\"\n",
    "    instance_accuracy = []\n",
    "    for idx in range(len(predicted_labels)): \n",
    "        ground_val = ast.literal_eval(ground_labels.iloc[idx])\n",
    "        intersection_vals = [set(tuple(row) for row in predicted_labels[idx] ) & set(tuple(row) for row in ground_val)]\n",
    "        if intersection_vals == [set()]:\n",
    "            instance_accuracy.append(0)\n",
    "        else:\n",
    "            if len(ground_val) == 0:\n",
    "                instance_accuracy.append(0)\n",
    "            else:\n",
    "                score = (len(intersection_vals))/len(ground_val)\n",
    "                instance_accuracy.append(score*100)\n",
    "        \n",
    "\n",
    "    return instance_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 values extracted!\n",
      "200 values extracted!\n",
      "300 values extracted!\n",
      "400 values extracted!\n",
      "500 values extracted!\n",
      "600 values extracted!\n",
      "700 values extracted!\n",
      "800 values extracted!\n",
      "The average accuracy for the dataset test_df is 0.58 %.\n"
     ]
    }
   ],
   "source": [
    "### Driver Section ###\n",
    "\n",
    "\"\"\"\n",
    "Please select the dataframe you want to evaluate from - train_df, valid_df,test_df\n",
    "Default = test_df\n",
    "Compute : On avgerage for computing 100 data points it takes about 3 minutes. \n",
    "\n",
    "\"\"\"\n",
    "df = test_df\n",
    "df_name = \"test_df\" # kindly enter the dataframe's name here\n",
    "predicted_labels = PredictTriples(df)\n",
    "ground_labels = df.labels\n",
    "#print(len(predicted_labels),len(ground_labels))\n",
    "if len(predicted_labels) == len(ground_labels):\n",
    "    instance_accuracy = CalculateAccuracy(predicted_labels,ground_labels)\n",
    "else :\n",
    "    print(\"Dimension mismatch between the predicted and ground labels\")\n",
    "average_accuracy = round(sum(instance_accuracy)/len(instance_accuracy),2)\n",
    "print(\"The average accuracy for the dataset {} is {} %.\".format(df_name,average_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 values extracted!\n",
      "200 values extracted!\n",
      "300 values extracted!\n",
      "400 values extracted!\n",
      "500 values extracted!\n",
      "600 values extracted!\n",
      "700 values extracted!\n",
      "The average accuracy for the dataset valid_df is 0.67 %.\n"
     ]
    }
   ],
   "source": [
    "### Driver Section ###\n",
    "\n",
    "\"\"\"\n",
    "Please select the dataframe you want to evaluate from - train_df, valid_df,test_df\n",
    "Default = test_df\n",
    "Compute : On avgerage for computing 100 data points it takes about 3 minutes. \n",
    "\n",
    "\"\"\"\n",
    "df = valid_df\n",
    "df_name = \"valid_df\" # kindly enter the dataframe's name here\n",
    "predicted_labels = PredictTriples(df)\n",
    "ground_labels = df.labels\n",
    "#print(len(predicted_labels),len(ground_labels))\n",
    "if len(predicted_labels) == len(ground_labels):\n",
    "    instance_accuracy = CalculateAccuracy(predicted_labels,ground_labels)\n",
    "else :\n",
    "    print(\"Dimension mismatch between the predicted and ground labels\")\n",
    "average_accuracy = round(sum(instance_accuracy)/len(instance_accuracy),2)\n",
    "\n",
    "#print(predicted_labels)\n",
    "#print(instance_accuracy)\n",
    "print(\"The average accuracy for the dataset {} is {} %.\".format(df_name,average_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
